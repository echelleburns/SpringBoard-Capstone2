{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Generator\n",
    "\n",
    "This notebook goes over how to create a data generator that reads in a section of a dataset and trains the model sequentially. \n",
    "\n",
    "TensorFlow wasn't loading properly on my Windows computer, so I followed this link to create a new environment to run tensorflow things: https://medium.com/@mengjiunchiou/how-to-set-keras-with-tensorflow-with-conda-virtual-environment-on-ubuntu-5d56d22e3dc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\echel\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\echel\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\echel\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\echel\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\echel\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\echel\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from skimage import io, filters, measure\n",
    "from scipy import ndimage\n",
    "from keras.models import Sequential\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image # for conversion to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data have been pre-processed already. The raw images were converted\n",
    "# to grayscale and were split into 25 smaller images. Labeled images\n",
    "# were created from the original images and are a series of 0 (no \n",
    "# person) and 255 (a person). The labeled images were also split\n",
    "# into 25 pieces so that they would match the original images. \n",
    "\n",
    "# Find location of image files and labeled images\n",
    "data = glob('data/raw/resized/with_people/splits/*hsv*.png')\n",
    "labels = glob('data/processed/dots/with_people/splits/*.png')\n",
    "\n",
    "# Split into the training and testing data\n",
    "train_X, test_X = train_test_split(data, test_size=0.25, random_state=33)\n",
    "train_Y, test_Y = train_test_split(labels, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to make a data generator for the model\n",
    "def gather_images(images, labels, batch_size=10): \n",
    "    \"\"\" Takes the original and labeled images, combines them into np \"\"\"\n",
    "    \"\"\" arrays, and passes to model\"\"\"\n",
    "    while 1: \n",
    "        for offset in range(0, len(images), batch_size): \n",
    "            X = [] # empty list for training data\n",
    "            Y = [] # empty list for labels \n",
    "            for img in images[offset:offset+batch_size]: # for each image in the list\n",
    "                img_temp = cv2.imread(img)\n",
    "                #img_temp = Image.open(img).convert('LA')\n",
    "                #img_temp = cv2.cvtColor(img_temp, cv2.COLOR_BGR2HSV)\n",
    "                img_flatten = np.array(img_temp) # create np array\n",
    "                X.append(img_flatten) # and add to list for X\n",
    "            for lab in labels[offset:offset+batch_size]: # for each label in the list\n",
    "                label_temp = io.imread(lab, as_gray=True)\n",
    "                labels_temp = measure.label(label_temp)\n",
    "                label_flatten = labels_temp.max() # create np array\n",
    "                Y.append(label_flatten) # and add to list for y\n",
    "            yield (np.array(X), np.array(Y).reshape(len(Y),1)) # yield X and y for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size = 30, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\echel\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "325/325 [==============================]325/325 [==============================] - 180s 553ms/step - loss: 15.1383 - mean_squared_error: 15.1383 - mean_absolute_error: 0.7792\n",
      "\n",
      "Epoch 2/10\n",
      "325/325 [==============================]325/325 [==============================] - 181s 558ms/step - loss: 0.3003 - mean_squared_error: 0.3003 - mean_absolute_error: 0.2466\n",
      "\n",
      "Epoch 3/10\n",
      "325/325 [==============================]325/325 [==============================] - 192s 592ms/step - loss: 0.2916 - mean_squared_error: 0.2916 - mean_absolute_error: 0.2476\n",
      "\n",
      "Epoch 4/10\n",
      "325/325 [==============================]325/325 [==============================] - 186s 571ms/step - loss: 0.2894 - mean_squared_error: 0.2894 - mean_absolute_error: 0.2412\n",
      "\n",
      "Epoch 5/10\n",
      "325/325 [==============================]325/325 [==============================] - 191s 587ms/step - loss: 0.2864 - mean_squared_error: 0.2864 - mean_absolute_error: 0.2433\n",
      "\n",
      "Epoch 6/10\n",
      "325/325 [==============================]325/325 [==============================] - 187s 574ms/step - loss: 0.2765 - mean_squared_error: 0.2765 - mean_absolute_error: 0.2344\n",
      "\n",
      "Epoch 7/10\n",
      "325/325 [==============================]325/325 [==============================] - 185s 571ms/step - loss: 0.2745 - mean_squared_error: 0.2745 - mean_absolute_error: 0.2341\n",
      "\n",
      "Epoch 8/10\n",
      "325/325 [==============================]325/325 [==============================] - 182s 559ms/step - loss: 0.2626 - mean_squared_error: 0.2626 - mean_absolute_error: 0.2264\n",
      "\n",
      "Epoch 9/10\n",
      "325/325 [==============================]325/325 [==============================] - 181s 558ms/step - loss: 0.2548 - mean_squared_error: 0.2548 - mean_absolute_error: 0.2257\n",
      "\n",
      "Epoch 10/10\n",
      "325/325 [==============================]325/325 [==============================] - 178s 547ms/step - loss: 0.2415 - mean_squared_error: 0.2415 - mean_absolute_error: 0.2205\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x244f5669a58>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size = 30, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling, Conv # 3 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\echel\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/10\n",
      "325/325 [==============================]325/325 [==============================] - 226s 694ms/step - loss: 1.2003 - mean_squared_error: 1.2003 - mean_absolute_error: 0.3623\n",
      "\n",
      "Epoch 2/10\n",
      "325/325 [==============================]325/325 [==============================] - 204s 627ms/step - loss: 0.2871 - mean_squared_error: 0.2871 - mean_absolute_error: 0.2426\n",
      "\n",
      "Epoch 3/10\n",
      "325/325 [==============================]325/325 [==============================] - 210s 647ms/step - loss: 0.2747 - mean_squared_error: 0.2747 - mean_absolute_error: 0.2278\n",
      "\n",
      "Epoch 4/10\n",
      "325/325 [==============================]325/325 [==============================] - 197s 605ms/step - loss: 0.2520 - mean_squared_error: 0.2520 - mean_absolute_error: 0.2117\n",
      "\n",
      "Epoch 5/10\n",
      "325/325 [==============================]325/325 [==============================] - 181s 556ms/step - loss: 0.2418 - mean_squared_error: 0.2418 - mean_absolute_error: 0.2045\n",
      "\n",
      "Epoch 6/10\n",
      "325/325 [==============================]325/325 [==============================] - 180s 553ms/step - loss: 0.2359 - mean_squared_error: 0.2359 - mean_absolute_error: 0.1989\n",
      "\n",
      "Epoch 7/10\n",
      "325/325 [==============================]325/325 [==============================] - 177s 545ms/step - loss: 0.2160 - mean_squared_error: 0.2160 - mean_absolute_error: 0.1864\n",
      "\n",
      "Epoch 8/10\n",
      "325/325 [==============================]325/325 [==============================] - 176s 541ms/step - loss: 0.2091 - mean_squared_error: 0.2091 - mean_absolute_error: 0.1831\n",
      "\n",
      "Epoch 9/10\n",
      "325/325 [==============================]325/325 [==============================] - 176s 543ms/step - loss: 0.2152 - mean_squared_error: 0.2152 - mean_absolute_error: 0.1827\n",
      "\n",
      "Epoch 10/10\n",
      "325/325 [==============================]325/325 [==============================] - 176s 542ms/step - loss: 0.2014 - mean_squared_error: 0.2014 - mean_absolute_error: 0.1731\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x2a71adabac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size = 50, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling, Conv # 3 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "195/195 [==============================]195/195 [==============================] - 174s 891ms/step - loss: 3.0571 - mean_squared_error: 3.0571 - mean_absolute_error: 0.5316\n",
      "\n",
      "Epoch 2/10\n",
      "195/195 [==============================]195/195 [==============================] - 173s 886ms/step - loss: 0.2951 - mean_squared_error: 0.2951 - mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 3/10\n",
      "195/195 [==============================]195/195 [==============================] - 172s 881ms/step - loss: 0.2852 - mean_squared_error: 0.2852 - mean_absolute_error: 0.2394\n",
      "\n",
      "Epoch 4/10\n",
      "195/195 [==============================]195/195 [==============================] - 177s 905ms/step - loss: 0.2722 - mean_squared_error: 0.2722 - mean_absolute_error: 0.2314\n",
      "\n",
      "Epoch 5/10\n",
      "195/195 [==============================]195/195 [==============================] - 179s 919ms/step - loss: 0.2660 - mean_squared_error: 0.2660 - mean_absolute_error: 0.2262\n",
      "\n",
      "Epoch 6/10\n",
      "195/195 [==============================]195/195 [==============================] - 172s 883ms/step - loss: 0.2596 - mean_squared_error: 0.2596 - mean_absolute_error: 0.2217\n",
      "\n",
      "Epoch 7/10\n",
      "195/195 [==============================]195/195 [==============================] - 172s 884ms/step - loss: 0.2484 - mean_squared_error: 0.2484 - mean_absolute_error: 0.2174\n",
      "\n",
      "Epoch 8/10\n",
      "195/195 [==============================]195/195 [==============================] - 172s 881ms/step - loss: 0.2376 - mean_squared_error: 0.2376 - mean_absolute_error: 0.2087\n",
      "\n",
      "Epoch 9/10\n",
      "195/195 [==============================]195/195 [==============================] - 175s 900ms/step - loss: 0.2370 - mean_squared_error: 0.2370 - mean_absolute_error: 0.2046\n",
      "\n",
      "Epoch 10/10\n",
      "195/195 [==============================]195/195 [==============================] - 179s 919ms/step - loss: 0.2430 - mean_squared_error: 0.2430 - mean_absolute_error: 0.2086\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x2a71ce77390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size = 50, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "195/195 [==============================]195/195 [==============================] - 178s 914ms/step - loss: 17.6068 - mean_squared_error: 17.6068 - mean_absolute_error: 0.9950\n",
      "\n",
      "Epoch 2/10\n",
      "195/195 [==============================]195/195 [==============================] - 184s 942ms/step - loss: 0.3048 - mean_squared_error: 0.3048 - mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 3/10\n",
      "195/195 [==============================]195/195 [==============================] - 177s 907ms/step - loss: 0.2909 - mean_squared_error: 0.2909 - mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 4/10\n",
      "195/195 [==============================]195/195 [==============================] - 179s 917ms/step - loss: 0.2779 - mean_squared_error: 0.2779 - mean_absolute_error: 0.2404\n",
      "\n",
      "Epoch 5/10\n",
      "195/195 [==============================]195/195 [==============================] - 182s 934ms/step - loss: 0.2717 - mean_squared_error: 0.2717 - mean_absolute_error: 0.2283\n",
      "\n",
      "Epoch 6/10\n",
      "195/195 [==============================]195/195 [==============================] - 177s 906ms/step - loss: 0.2558 - mean_squared_error: 0.2558 - mean_absolute_error: 0.2185\n",
      "\n",
      "Epoch 7/10\n",
      "195/195 [==============================]195/195 [==============================] - 176s 901ms/step - loss: 0.2451 - mean_squared_error: 0.2451 - mean_absolute_error: 0.2134\n",
      "\n",
      "Epoch 8/10\n",
      "195/195 [==============================]195/195 [==============================] - 190s 976ms/step - loss: 0.2487 - mean_squared_error: 0.2487 - mean_absolute_error: 0.2135\n",
      "\n",
      "Epoch 9/10\n",
      "195/195 [==============================]195/195 [==============================] - 193s 989ms/step - loss: 0.2298 - mean_squared_error: 0.2298 - mean_absolute_error: 0.2016\n",
      "\n",
      "Epoch 10/10\n",
      "195/195 [==============================]195/195 [==============================] - 189s 970ms/step - loss: 0.2245 - mean_squared_error: 0.2245 - mean_absolute_error: 0.1968\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x244879e7748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size = 30, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "325/325 [==============================]325/325 [==============================] - 416s 1s/step - loss: 3.5330 - mean_squared_error: 3.5330 - mean_absolute_error: 0.4980\n",
      "\n",
      "Epoch 2/10\n",
      "325/325 [==============================]325/325 [==============================] - 425s 1s/step - loss: 0.2964 - mean_squared_error: 0.2964 - mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 3/10\n",
      "325/325 [==============================]325/325 [==============================] - 398s 1s/step - loss: 0.2901 - mean_squared_error: 0.2901 - mean_absolute_error: 0.2491\n",
      "\n",
      "Epoch 4/10\n",
      "325/325 [==============================]325/325 [==============================] - 393s 1s/step - loss: 0.2842 - mean_squared_error: 0.2842 - mean_absolute_error: 0.2399\n",
      "\n",
      "Epoch 5/10\n",
      "325/325 [==============================]325/325 [==============================] - 410s 1s/step - loss: 0.2755 - mean_squared_error: 0.2755 - mean_absolute_error: 0.2345\n",
      "\n",
      "Epoch 6/10\n",
      "325/325 [==============================]325/325 [==============================] - 392s 1s/step - loss: 0.2606 - mean_squared_error: 0.2606 - mean_absolute_error: 0.2239\n",
      "\n",
      "Epoch 7/10\n",
      "325/325 [==============================]325/325 [==============================] - 401s 1s/step - loss: 0.2536 - mean_squared_error: 0.2536 - mean_absolute_error: 0.2167\n",
      "\n",
      "Epoch 8/10\n",
      "325/325 [==============================]325/325 [==============================] - 401s 1s/step - loss: 0.2421 - mean_squared_error: 0.2421 - mean_absolute_error: 0.2115\n",
      "\n",
      "Epoch 9/10\n",
      "325/325 [==============================]325/325 [==============================] - 390s 1s/step - loss: 0.2535 - mean_squared_error: 0.2535 - mean_absolute_error: 0.2124\n",
      "\n",
      "Epoch 10/10\n",
      "325/325 [==============================]325/325 [==============================] - 403s 1s/step - loss: 0.2379 - mean_squared_error: 0.2379 - mean_absolute_error: 0.2045\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x24488a30240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size = 30, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 10x10 with 5x5 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "325/325 [==============================]325/325 [==============================] - 412s 1s/step - loss: 1.7279 - mean_squared_error: 1.7279 - mean_absolute_error: 0.3966\n",
      "\n",
      "Epoch 2/10\n",
      "325/325 [==============================]325/325 [==============================] - 406s 1s/step - loss: 0.2950 - mean_squared_error: 0.2950 - mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 3/10\n",
      "325/325 [==============================]325/325 [==============================] - 416s 1s/step - loss: 0.2912 - mean_squared_error: 0.2912 - mean_absolute_error: 0.2432\n",
      "\n",
      "Epoch 4/10\n",
      "325/325 [==============================]325/325 [==============================] - 427s 1s/step - loss: 0.2857 - mean_squared_error: 0.2857 - mean_absolute_error: 0.2410\n",
      "\n",
      "Epoch 5/10\n",
      "325/325 [==============================]325/325 [==============================] - 423s 1s/step - loss: 0.2765 - mean_squared_error: 0.2765 - mean_absolute_error: 0.2344\n",
      "\n",
      "Epoch 6/10\n",
      "325/325 [==============================]325/325 [==============================] - 415s 1s/step - loss: 0.2930 - mean_squared_error: 0.2930 - mean_absolute_error: 0.2386\n",
      "\n",
      "Epoch 7/10\n",
      "325/325 [==============================]325/325 [==============================] - 406s 1s/step - loss: 0.2849 - mean_squared_error: 0.2849 - mean_absolute_error: 0.2377\n",
      "\n",
      "Epoch 8/10\n",
      "325/325 [==============================]325/325 [==============================] - 407s 1s/step - loss: 0.2771 - mean_squared_error: 0.2771 - mean_absolute_error: 0.2336\n",
      "\n",
      "Epoch 9/10\n",
      "325/325 [==============================]325/325 [==============================] - 409s 1s/step - loss: 0.2717 - mean_squared_error: 0.2717 - mean_absolute_error: 0.2342\n",
      "\n",
      "Epoch 10/10\n",
      "325/325 [==============================]325/325 [==============================] - 410s 1s/step - loss: 0.2614 - mean_squared_error: 0.2614 - mean_absolute_error: 0.2321\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x24487b2e358>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (10,10), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size = 50, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 10x10 with 5x5 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "195/195 [==============================]195/195 [==============================] - 405s 2s/step - loss: 2.9963 - mean_squared_error: 2.9963 - mean_absolute_error: 0.4759\n",
      "\n",
      "Epoch 2/10\n",
      "195/195 [==============================]195/195 [==============================] - 400s 2s/step - loss: 0.2990 - mean_squared_error: 0.2990 - mean_absolute_error: 0.2558\n",
      "\n",
      "Epoch 3/10\n",
      "195/195 [==============================]195/195 [==============================] - 405s 2s/step - loss: 0.2997 - mean_squared_error: 0.2997 - mean_absolute_error: 0.2579\n",
      "\n",
      "Epoch 4/10\n",
      "195/195 [==============================]195/195 [==============================] - 434s 2s/step - loss: 0.2995 - mean_squared_error: 0.2995 - mean_absolute_error: 0.2580\n",
      "\n",
      "Epoch 5/10\n",
      "195/195 [==============================]195/195 [==============================] - 433s 2s/step - loss: 0.2990 - mean_squared_error: 0.2990 - mean_absolute_error: 0.2580\n",
      "\n",
      "Epoch 6/10\n",
      "195/195 [==============================]195/195 [==============================] - 415s 2s/step - loss: 0.2990 - mean_squared_error: 0.2990 - mean_absolute_error: 0.2575\n",
      "\n",
      "Epoch 7/10\n",
      "195/195 [==============================]195/195 [==============================] - 416s 2s/step - loss: 0.3000 - mean_squared_error: 0.3000 - mean_absolute_error: 0.2580\n",
      "\n",
      "Epoch 8/10\n",
      "195/195 [==============================]195/195 [==============================] - 406s 2s/step - loss: 0.2989 - mean_squared_error: 0.2989 - mean_absolute_error: 0.2574\n",
      "\n",
      "Epoch 9/10\n",
      "195/195 [==============================]195/195 [==============================] - 406s 2s/step - loss: 0.2985 - mean_squared_error: 0.2985 - mean_absolute_error: 0.2585\n",
      "\n",
      "Epoch 10/10\n",
      "195/195 [==============================]195/195 [==============================] - 425s 2s/step - loss: 0.2986 - mean_squared_error: 0.2986 - mean_absolute_error: 0.2573\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x24487b92908>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (10,10), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size = 30, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 10x10 with 5x5 pooling, Conv # 3 = 5x5 with 2x2 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "325/325 [==============================]325/325 [==============================] - 474s 1s/step - loss: 0.5884 - mean_squared_error: 0.5884 - mean_absolute_error: 0.2957\n",
      "\n",
      "Epoch 2/10\n",
      "325/325 [==============================]325/325 [==============================] - 426s 1s/step - loss: 0.2975 - mean_squared_error: 0.2975 - mean_absolute_error: 0.2553\n",
      "\n",
      "Epoch 3/10\n",
      "325/325 [==============================]325/325 [==============================] - 422s 1s/step - loss: 0.2962 - mean_squared_error: 0.2962 - mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 4/10\n",
      "325/325 [==============================]325/325 [==============================] - 421s 1s/step - loss: 0.2976 - mean_squared_error: 0.2976 - mean_absolute_error: 0.2548\n",
      "\n",
      "Epoch 5/10\n",
      "325/325 [==============================]325/325 [==============================] - 438s 1s/step - loss: 0.2969 - mean_squared_error: 0.2969 - mean_absolute_error: 0.2577\n",
      "\n",
      "Epoch 6/10\n",
      "325/325 [==============================]325/325 [==============================] - 426s 1s/step - loss: 0.2969 - mean_squared_error: 0.2969 - mean_absolute_error: 0.2567\n",
      "\n",
      "Epoch 7/10\n",
      "325/325 [==============================]325/325 [==============================] - 430s 1s/step - loss: 0.2965 - mean_squared_error: 0.2965 - mean_absolute_error: 0.2559\n",
      "\n",
      "Epoch 8/10\n",
      "325/325 [==============================]325/325 [==============================] - 430s 1s/step - loss: 0.2971 - mean_squared_error: 0.2971 - mean_absolute_error: 0.2571\n",
      "\n",
      "Epoch 9/10\n",
      "325/325 [==============================]325/325 [==============================] - 443s 1s/step - loss: 0.2968 - mean_squared_error: 0.2968 - mean_absolute_error: 0.2564\n",
      "\n",
      "Epoch 10/10\n",
      "325/325 [==============================]325/325 [==============================] - 441s 1s/step - loss: 0.2974 - mean_squared_error: 0.2974 - mean_absolute_error: 0.2574\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x24487c28908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (10,10), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(32, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06639786]\n",
      " [0.597787  ]\n",
      " [0.05902122]\n",
      " [0.04196331]\n",
      " [0.04343889]\n",
      " [0.25613928]\n",
      " [0.04196331]\n",
      " [0.12044369]\n",
      " [0.04196331]\n",
      " [0.04298855]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(gather_images(test_X, test_Y, batch_size=batch_size), len(test_X)//batch_size)\n",
    "print(predictions[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "test_labels = next(gather_images(test_X, test_Y, batch_size=len(test_X)))[1]\n",
    "print(test_labels[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_notebook",
   "language": "python",
   "name": "tensor_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
