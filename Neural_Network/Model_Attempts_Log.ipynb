{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Refining Models\n",
    "\n",
    "This notebook shows a variety of attempts at finding the best model to count people in drone footage.\n",
    "\n",
    "TensorFlow wasn't loading properly on my Windows computer, so I followed this link to create a new environment to run tensorflow things: https://medium.com/@mengjiunchiou/how-to-set-keras-with-tensorflow-with-conda-virtual-environment-on-ubuntu-5d56d22e3dc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "# import the necessary packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from skimage import io, filters, measure\n",
    "from scipy import ndimage\n",
    "from keras.models import Sequential\n",
    "import cv2\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image # for conversion to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data have been pre-processed already. The raw images were converted\n",
    "# to grayscale and were split into 25 smaller images. Labeled images\n",
    "# were created from the original images and are a series of 0 (no \n",
    "# person) and 255 (a person). The labeled images were also split\n",
    "# into 25 pieces so that they would match the original images. \n",
    "\n",
    "# Find location of image files and labeled images\n",
    "data = glob('data/raw/resized/with_people/splits/*hsv*.png')\n",
    "labels = glob('data/processed/dots/with_people/splits/*.png')\n",
    "\n",
    "# Split into the training and testing data\n",
    "train_X, test_X = train_test_split(data, test_size=0.2, random_state=33)\n",
    "train_Y, test_Y = train_test_split(labels, test_size=0.2, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to make a data generator for the model\n",
    "def gather_images(images, labels, batch_size=10): \n",
    "    \"\"\" Takes the original and labeled images, combines them into np \"\"\"\n",
    "    \"\"\" arrays, and passes to model\"\"\"\n",
    "    while 1: \n",
    "        for offset in range(0, len(images), batch_size): \n",
    "            X = [] # empty list for training data\n",
    "            Y = [] # empty list for labels \n",
    "            for img in images[offset:offset+batch_size]: # for each image in the list\n",
    "                img_temp = cv2.imread(img)\n",
    "                img_flatten = np.array(img_temp)# [:,:,2:3]) # create np array [0:108, 0:192, 0:1]\n",
    "                X.append(img_flatten) # and add to list for X\n",
    "            for lab in labels[offset:offset+batch_size]: # for each label in the list\n",
    "                label_temp = io.imread(lab, as_gray=True)\n",
    "                labels_temp = measure.label(label_temp)\n",
    "                label_flatten = labels_temp.max() # create np array\n",
    "                Y.append(label_flatten) # and add to list for y\n",
    "            yield (np.array(X), np.array(Y).reshape(len(Y),1)) # yield X and y for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a Normalization Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the all channels from HSV, Batch Size = 30, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 243s 701ms/step - loss: 0.5793 - mean_squared_error: 0.5793 - mean_absolute_error: 0.3062 - val_loss: 0.2695 - val_mean_squared_error: 0.2695 - val_mean_absolute_error: 0.2395\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 213s 614ms/step - loss: 0.2984 - mean_squared_error: 0.2984 - mean_absolute_error: 0.2514 - val_loss: 0.2592 - val_mean_squared_error: 0.2592 - val_mean_absolute_error: 0.2483\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 214s 616ms/step - loss: 0.2815 - mean_squared_error: 0.2815 - mean_absolute_error: 0.2356 - val_loss: 0.2386 - val_mean_squared_error: 0.2386 - val_mean_absolute_error: 0.1980\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 211s 608ms/step - loss: 0.2594 - mean_squared_error: 0.2594 - mean_absolute_error: 0.2136 - val_loss: 0.2404 - val_mean_squared_error: 0.2404 - val_mean_absolute_error: 0.1834\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 212s 611ms/step - loss: 0.2397 - mean_squared_error: 0.2397 - mean_absolute_error: 0.2013 - val_loss: 0.2202 - val_mean_squared_error: 0.2202 - val_mean_absolute_error: 0.1819\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 213s 613ms/step - loss: 0.2287 - mean_squared_error: 0.2287 - mean_absolute_error: 0.1902 - val_loss: 0.2164 - val_mean_squared_error: 0.2164 - val_mean_absolute_error: 0.1883\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 211s 607ms/step - loss: 0.2099 - mean_squared_error: 0.2099 - mean_absolute_error: 0.1830 - val_loss: 0.1839 - val_mean_squared_error: 0.1839 - val_mean_absolute_error: 0.1660\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 211s 609ms/step - loss: 0.1961 - mean_squared_error: 0.1961 - mean_absolute_error: 0.1705 - val_loss: 0.1701 - val_mean_squared_error: 0.1701 - val_mean_absolute_error: 0.1633\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 212s 612ms/step - loss: 0.1915 - mean_squared_error: 0.1915 - mean_absolute_error: 0.1649 - val_loss: 0.1773 - val_mean_squared_error: 0.1773 - val_mean_absolute_error: 0.1780\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 211s 608ms/step - loss: 0.1843 - mean_squared_error: 0.1843 - mean_absolute_error: 0.1618 - val_loss: 0.2130 - val_mean_squared_error: 0.2130 - val_mean_absolute_error: 0.2513\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x178801c4048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the all channels from HSV, Batch Size = 30, Normalization layer, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 310s 893ms/step - loss: 0.2641 - mean_squared_error: 0.2641 - mean_absolute_error: 0.2226 - val_loss: 0.1829 - val_mean_squared_error: 0.1829 - val_mean_absolute_error: 0.1546\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 307s 884ms/step - loss: 0.2110 - mean_squared_error: 0.2110 - mean_absolute_error: 0.1780 - val_loss: 0.1721 - val_mean_squared_error: 0.1721 - val_mean_absolute_error: 0.1539\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 310s 894ms/step - loss: 0.1971 - mean_squared_error: 0.1971 - mean_absolute_error: 0.1639 - val_loss: 0.1961 - val_mean_squared_error: 0.1961 - val_mean_absolute_error: 0.1715\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 305s 879ms/step - loss: 0.1853 - mean_squared_error: 0.1853 - mean_absolute_error: 0.1629 - val_loss: 0.1604 - val_mean_squared_error: 0.1604 - val_mean_absolute_error: 0.1675\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 306s 883ms/step - loss: 0.1744 - mean_squared_error: 0.1744 - mean_absolute_error: 0.1523 - val_loss: 0.1843 - val_mean_squared_error: 0.1843 - val_mean_absolute_error: 0.1531\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 307s 883ms/step - loss: 0.1609 - mean_squared_error: 0.1609 - mean_absolute_error: 0.1493 - val_loss: 0.1512 - val_mean_squared_error: 0.1512 - val_mean_absolute_error: 0.1365\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 308s 889ms/step - loss: 0.1554 - mean_squared_error: 0.1554 - mean_absolute_error: 0.1420 - val_loss: 0.1603 - val_mean_squared_error: 0.1603 - val_mean_absolute_error: 0.1747\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 337s 970ms/step - loss: 0.1482 - mean_squared_error: 0.1482 - mean_absolute_error: 0.1424 - val_loss: 0.1663 - val_mean_squared_error: 0.1663 - val_mean_absolute_error: 0.1637\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 334s 964ms/step - loss: 0.1399 - mean_squared_error: 0.1399 - mean_absolute_error: 0.1381 - val_loss: 0.1487 - val_mean_squared_error: 0.1487 - val_mean_absolute_error: 0.1706\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 334s 962ms/step - loss: 0.1263 - mean_squared_error: 0.1263 - mean_absolute_error: 0.1326 - val_loss: 0.1428 - val_mean_squared_error: 0.1428 - val_mean_absolute_error: 0.1430\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x17880b83240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,3)),\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Different HSV Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the first channel from HSV, Batch Size = 30, Normalization layer, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 215s 618ms/step - loss: 0.3042 - mean_squared_error: 0.3042 - mean_absolute_error: 0.2493 - val_loss: 0.2590 - val_mean_squared_error: 0.2590 - val_mean_absolute_error: 0.2260\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 180s 518ms/step - loss: 0.2792 - mean_squared_error: 0.2792 - mean_absolute_error: 0.2305 - val_loss: 0.2314 - val_mean_squared_error: 0.2314 - val_mean_absolute_error: 0.1905\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 174s 501ms/step - loss: 0.2548 - mean_squared_error: 0.2548 - mean_absolute_error: 0.2143 - val_loss: 0.2368 - val_mean_squared_error: 0.2368 - val_mean_absolute_error: 0.2039\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 174s 500ms/step - loss: 0.2396 - mean_squared_error: 0.2396 - mean_absolute_error: 0.2046 - val_loss: 0.2494 - val_mean_squared_error: 0.2494 - val_mean_absolute_error: 0.2435\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 173s 498ms/step - loss: 0.2285 - mean_squared_error: 0.2285 - mean_absolute_error: 0.2014 - val_loss: 0.2359 - val_mean_squared_error: 0.2359 - val_mean_absolute_error: 0.1986\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 186s 536ms/step - loss: 0.2163 - mean_squared_error: 0.2163 - mean_absolute_error: 0.1970 - val_loss: 0.2726 - val_mean_squared_error: 0.2726 - val_mean_absolute_error: 0.2160\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 171s 494ms/step - loss: 0.2032 - mean_squared_error: 0.2032 - mean_absolute_error: 0.1900 - val_loss: 0.2758 - val_mean_squared_error: 0.2758 - val_mean_absolute_error: 0.2229\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 170s 491ms/step - loss: 0.1987 - mean_squared_error: 0.1987 - mean_absolute_error: 0.1953 - val_loss: 0.2678 - val_mean_squared_error: 0.2678 - val_mean_absolute_error: 0.2282\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 175s 505ms/step - loss: 0.1844 - mean_squared_error: 0.1844 - mean_absolute_error: 0.1876 - val_loss: 0.2746 - val_mean_squared_error: 0.2746 - val_mean_absolute_error: 0.2350\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 170s 491ms/step - loss: 0.1970 - mean_squared_error: 0.1970 - mean_absolute_error: 0.1898 - val_loss: 0.2529 - val_mean_squared_error: 0.2529 - val_mean_absolute_error: 0.2085\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cbf2db7f28>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the second channel from HSV, Batch Size = 30, Normalization layer, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 178s 512ms/step - loss: 0.2521 - mean_squared_error: 0.2521 - mean_absolute_error: 0.2102 - val_loss: 0.1915 - val_mean_squared_error: 0.1915 - val_mean_absolute_error: 0.1471\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 174s 503ms/step - loss: 0.2053 - mean_squared_error: 0.2053 - mean_absolute_error: 0.1718 - val_loss: 0.1694 - val_mean_squared_error: 0.1694 - val_mean_absolute_error: 0.1365\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 173s 499ms/step - loss: 0.1699 - mean_squared_error: 0.1699 - mean_absolute_error: 0.1492 - val_loss: 0.1529 - val_mean_squared_error: 0.1529 - val_mean_absolute_error: 0.1496\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 180s 520ms/step - loss: 0.1543 - mean_squared_error: 0.1543 - mean_absolute_error: 0.1404 - val_loss: 0.1568 - val_mean_squared_error: 0.1568 - val_mean_absolute_error: 0.1296\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 179s 515ms/step - loss: 0.1360 - mean_squared_error: 0.1360 - mean_absolute_error: 0.1332 - val_loss: 0.1621 - val_mean_squared_error: 0.1621 - val_mean_absolute_error: 0.1271\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 182s 524ms/step - loss: 0.1241 - mean_squared_error: 0.1241 - mean_absolute_error: 0.1318 - val_loss: 0.1579 - val_mean_squared_error: 0.1579 - val_mean_absolute_error: 0.1486\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 179s 515ms/step - loss: 0.1304 - mean_squared_error: 0.1304 - mean_absolute_error: 0.1366 - val_loss: 0.1668 - val_mean_squared_error: 0.1668 - val_mean_absolute_error: 0.1457\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 178s 513ms/step - loss: 0.1122 - mean_squared_error: 0.1122 - mean_absolute_error: 0.1277 - val_loss: 0.1503 - val_mean_squared_error: 0.1503 - val_mean_absolute_error: 0.1414\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 181s 520ms/step - loss: 0.0985 - mean_squared_error: 0.0985 - mean_absolute_error: 0.1215 - val_loss: 0.1731 - val_mean_squared_error: 0.1731 - val_mean_absolute_error: 0.1381\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 180s 518ms/step - loss: 0.1000 - mean_squared_error: 0.1000 - mean_absolute_error: 0.1255 - val_loss: 0.1628 - val_mean_squared_error: 0.1628 - val_mean_absolute_error: 0.1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cc0622dba8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the third channel from HSV, Batch Size = 30, Normalization layer, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 174s 500ms/step - loss: 0.3035 - mean_squared_error: 0.3035 - mean_absolute_error: 0.2511 - val_loss: 0.2528 - val_mean_squared_error: 0.2528 - val_mean_absolute_error: 0.1901\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 171s 492ms/step - loss: 0.2514 - mean_squared_error: 0.2514 - mean_absolute_error: 0.2074 - val_loss: 0.2209 - val_mean_squared_error: 0.2209 - val_mean_absolute_error: 0.1547\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 171s 492ms/step - loss: 0.2277 - mean_squared_error: 0.2277 - mean_absolute_error: 0.1831 - val_loss: 0.1946 - val_mean_squared_error: 0.1946 - val_mean_absolute_error: 0.1599\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 171s 492ms/step - loss: 0.2079 - mean_squared_error: 0.2079 - mean_absolute_error: 0.1660 - val_loss: 0.1740 - val_mean_squared_error: 0.1740 - val_mean_absolute_error: 0.1519\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 175s 503ms/step - loss: 0.1894 - mean_squared_error: 0.1894 - mean_absolute_error: 0.1574 - val_loss: 0.1733 - val_mean_squared_error: 0.1733 - val_mean_absolute_error: 0.1444\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 171s 492ms/step - loss: 0.1732 - mean_squared_error: 0.1732 - mean_absolute_error: 0.1528 - val_loss: 0.2085 - val_mean_squared_error: 0.2085 - val_mean_absolute_error: 0.1618\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 172s 495ms/step - loss: 0.1545 - mean_squared_error: 0.1545 - mean_absolute_error: 0.1462 - val_loss: 0.2004 - val_mean_squared_error: 0.2004 - val_mean_absolute_error: 0.1604\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 171s 493ms/step - loss: 0.1546 - mean_squared_error: 0.1546 - mean_absolute_error: 0.1477 - val_loss: 0.2473 - val_mean_squared_error: 0.2473 - val_mean_absolute_error: 0.2199\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 172s 495ms/step - loss: 0.1270 - mean_squared_error: 0.1270 - mean_absolute_error: 0.1380 - val_loss: 0.1930 - val_mean_squared_error: 0.1930 - val_mean_absolute_error: 0.1431\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 173s 499ms/step - loss: 0.1395 - mean_squared_error: 0.1395 - mean_absolute_error: 0.1419 - val_loss: 0.1978 - val_mean_squared_error: 0.1978 - val_mean_absolute_error: 0.1846\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cc0ef79438>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the first channel from HSV, Batch Size = 30, Normalization layer, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling, Dropout # 1 with 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 176s 506ms/step - loss: 0.3045 - mean_squared_error: 0.3045 - mean_absolute_error: 0.2489 - val_loss: 0.2591 - val_mean_squared_error: 0.2591 - val_mean_absolute_error: 0.2296\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 173s 498ms/step - loss: 0.2748 - mean_squared_error: 0.2748 - mean_absolute_error: 0.2334 - val_loss: 0.2329 - val_mean_squared_error: 0.2329 - val_mean_absolute_error: 0.2058\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 174s 500ms/step - loss: 0.2556 - mean_squared_error: 0.2556 - mean_absolute_error: 0.2195 - val_loss: 0.2333 - val_mean_squared_error: 0.2333 - val_mean_absolute_error: 0.2038\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 175s 503ms/step - loss: 0.2406 - mean_squared_error: 0.2406 - mean_absolute_error: 0.2092 - val_loss: 0.2505 - val_mean_squared_error: 0.2505 - val_mean_absolute_error: 0.2336\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 174s 503ms/step - loss: 0.2405 - mean_squared_error: 0.2405 - mean_absolute_error: 0.2043 - val_loss: 0.2334 - val_mean_squared_error: 0.2334 - val_mean_absolute_error: 0.2140\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 173s 499ms/step - loss: 0.2251 - mean_squared_error: 0.2251 - mean_absolute_error: 0.1992 - val_loss: 0.2499 - val_mean_squared_error: 0.2499 - val_mean_absolute_error: 0.2533\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 173s 498ms/step - loss: 0.2181 - mean_squared_error: 0.2181 - mean_absolute_error: 0.1965 - val_loss: 0.2611 - val_mean_squared_error: 0.2611 - val_mean_absolute_error: 0.2459\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 173s 498ms/step - loss: 0.2020 - mean_squared_error: 0.2020 - mean_absolute_error: 0.1886 - val_loss: 0.2462 - val_mean_squared_error: 0.2462 - val_mean_absolute_error: 0.2339\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 173s 499ms/step - loss: 0.1945 - mean_squared_error: 0.1945 - mean_absolute_error: 0.1826 - val_loss: 0.2223 - val_mean_squared_error: 0.2223 - val_mean_absolute_error: 0.2196\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 185s 532ms/step - loss: 0.1921 - mean_squared_error: 0.1921 - mean_absolute_error: 0.1855 - val_loss: 0.2354 - val_mean_squared_error: 0.2354 - val_mean_absolute_error: 0.2209\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cc00933cf8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the Nodes in the Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the first channel from HSV, Batch Size = 30, Normalization Layer, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling, all Conv layers with 64 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 911s 3s/step - loss: 0.3076 - mean_squared_error: 0.3076 - mean_absolute_error: 0.2493 - val_loss: 0.2689 - val_mean_squared_error: 0.2689 - val_mean_absolute_error: 0.2449\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 864s 2s/step - loss: 0.2906 - mean_squared_error: 0.2906 - mean_absolute_error: 0.2441 - val_loss: 0.2439 - val_mean_squared_error: 0.2439 - val_mean_absolute_error: 0.2004\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 841s 2s/step - loss: 0.2838 - mean_squared_error: 0.2838 - mean_absolute_error: 0.2356 - val_loss: 0.2620 - val_mean_squared_error: 0.2620 - val_mean_absolute_error: 0.2636\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 835s 2s/step - loss: 0.2900 - mean_squared_error: 0.2900 - mean_absolute_error: 0.2446 - val_loss: 0.2453 - val_mean_squared_error: 0.2453 - val_mean_absolute_error: 0.2219\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 860s 2s/step - loss: 0.2743 - mean_squared_error: 0.2743 - mean_absolute_error: 0.2314 - val_loss: 0.2415 - val_mean_squared_error: 0.2415 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 828s 2s/step - loss: 0.2635 - mean_squared_error: 0.2635 - mean_absolute_error: 0.2226 - val_loss: 0.2302 - val_mean_squared_error: 0.2302 - val_mean_absolute_error: 0.2412\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 836s 2s/step - loss: 0.2406 - mean_squared_error: 0.2406 - mean_absolute_error: 0.2084 - val_loss: 0.2426 - val_mean_squared_error: 0.2426 - val_mean_absolute_error: 0.2332\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 833s 2s/step - loss: 0.2343 - mean_squared_error: 0.2343 - mean_absolute_error: 0.2054 - val_loss: 0.2617 - val_mean_squared_error: 0.2617 - val_mean_absolute_error: 0.2455\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 844s 2s/step - loss: 0.2207 - mean_squared_error: 0.2207 - mean_absolute_error: 0.1954 - val_loss: 0.2631 - val_mean_squared_error: 0.2631 - val_mean_absolute_error: 0.2394\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 827s 2s/step - loss: 0.2510 - mean_squared_error: 0.2510 - mean_absolute_error: 0.2141 - val_loss: 0.2349 - val_mean_squared_error: 0.2349 - val_mean_absolute_error: 0.2399\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cbe2c4ab00>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(64, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(64, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the first channel from HSV, Batch Size = 30, Normalization Layer, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling, all Conv layers with 16 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 237s 684ms/step - loss: 0.3003 - mean_squared_error: 0.3003 - mean_absolute_error: 0.2492 - val_loss: 0.2421 - val_mean_squared_error: 0.2421 - val_mean_absolute_error: 0.2524\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 233s 672ms/step - loss: 0.2685 - mean_squared_error: 0.2685 - mean_absolute_error: 0.2256 - val_loss: 0.2339 - val_mean_squared_error: 0.2339 - val_mean_absolute_error: 0.1854\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 227s 654ms/step - loss: 0.2507 - mean_squared_error: 0.2507 - mean_absolute_error: 0.2113 - val_loss: 0.2278 - val_mean_squared_error: 0.2278 - val_mean_absolute_error: 0.2203\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 227s 654ms/step - loss: 0.2378 - mean_squared_error: 0.2378 - mean_absolute_error: 0.2053 - val_loss: 0.2577 - val_mean_squared_error: 0.2577 - val_mean_absolute_error: 0.2415\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 234s 675ms/step - loss: 0.2275 - mean_squared_error: 0.2275 - mean_absolute_error: 0.1991 - val_loss: 0.2423 - val_mean_squared_error: 0.2423 - val_mean_absolute_error: 0.2232\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 228s 656ms/step - loss: 0.2194 - mean_squared_error: 0.2194 - mean_absolute_error: 0.1946 - val_loss: 0.2340 - val_mean_squared_error: 0.2340 - val_mean_absolute_error: 0.1957\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 228s 658ms/step - loss: 0.2241 - mean_squared_error: 0.2241 - mean_absolute_error: 0.1983 - val_loss: 0.2514 - val_mean_squared_error: 0.2514 - val_mean_absolute_error: 0.2136\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 226s 652ms/step - loss: 0.2049 - mean_squared_error: 0.2049 - mean_absolute_error: 0.1858 - val_loss: 0.2476 - val_mean_squared_error: 0.2476 - val_mean_absolute_error: 0.2145\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 226s 650ms/step - loss: 0.1981 - mean_squared_error: 0.1981 - mean_absolute_error: 0.1829 - val_loss: 0.2316 - val_mean_squared_error: 0.2316 - val_mean_absolute_error: 0.2112\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 227s 653ms/step - loss: 0.2020 - mean_squared_error: 0.2020 - mean_absolute_error: 0.1892 - val_loss: 0.2445 - val_mean_squared_error: 0.2445 - val_mean_absolute_error: 0.2061\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cbfbcbca90>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(16, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the first channel from HSV, Batch Size = 30, Normaization layer, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling, all Conv layers with 32 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 428s 1s/step - loss: 0.3025 - mean_squared_error: 0.3025 - mean_absolute_error: 0.2474 - val_loss: 0.2614 - val_mean_squared_error: 0.2614 - val_mean_absolute_error: 0.2247\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 415s 1s/step - loss: 0.2811 - mean_squared_error: 0.2811 - mean_absolute_error: 0.2345 - val_loss: 0.2451 - val_mean_squared_error: 0.2451 - val_mean_absolute_error: 0.1913\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 437s 1s/step - loss: 0.2625 - mean_squared_error: 0.2625 - mean_absolute_error: 0.2200 - val_loss: 0.2305 - val_mean_squared_error: 0.2305 - val_mean_absolute_error: 0.2258\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 445s 1s/step - loss: 0.2397 - mean_squared_error: 0.2397 - mean_absolute_error: 0.2072 - val_loss: 0.2598 - val_mean_squared_error: 0.2598 - val_mean_absolute_error: 0.2555\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 454s 1s/step - loss: 0.2283 - mean_squared_error: 0.2283 - mean_absolute_error: 0.1997 - val_loss: 0.2771 - val_mean_squared_error: 0.2771 - val_mean_absolute_error: 0.2689\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 448s 1s/step - loss: 0.2213 - mean_squared_error: 0.2213 - mean_absolute_error: 0.1945 - val_loss: 0.2387 - val_mean_squared_error: 0.2387 - val_mean_absolute_error: 0.2292\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 422s 1s/step - loss: 0.2300 - mean_squared_error: 0.2300 - mean_absolute_error: 0.2036 - val_loss: 0.3026 - val_mean_squared_error: 0.3026 - val_mean_absolute_error: 0.2603\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 433s 1s/step - loss: 0.2218 - mean_squared_error: 0.2218 - mean_absolute_error: 0.1963 - val_loss: 0.2676 - val_mean_squared_error: 0.2676 - val_mean_absolute_error: 0.2311\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 420s 1s/step - loss: 0.2100 - mean_squared_error: 0.2100 - mean_absolute_error: 0.1918 - val_loss: 0.2607 - val_mean_squared_error: 0.2607 - val_mean_absolute_error: 0.2313\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 451s 1s/step - loss: 0.1962 - mean_squared_error: 0.1962 - mean_absolute_error: 0.1865 - val_loss: 0.2870 - val_mean_squared_error: 0.2870 - val_mean_absolute_error: 0.2378\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cbfbc87940>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(32, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using all channels from HSV, Batch Size = 50, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling, Conv # 3 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "195/195 [==============================]195/195 [==============================] - 174s 891ms/step - loss: 3.0571 - mean_squared_error: 3.0571 - mean_absolute_error: 0.5316\n",
      "\n",
      "Epoch 2/10\n",
      "195/195 [==============================]195/195 [==============================] - 173s 886ms/step - loss: 0.2951 - mean_squared_error: 0.2951 - mean_absolute_error: 0.2496\n",
      "\n",
      "Epoch 3/10\n",
      "195/195 [==============================]195/195 [==============================] - 172s 881ms/step - loss: 0.2852 - mean_squared_error: 0.2852 - mean_absolute_error: 0.2394\n",
      "\n",
      "Epoch 4/10\n",
      "195/195 [==============================]195/195 [==============================] - 177s 905ms/step - loss: 0.2722 - mean_squared_error: 0.2722 - mean_absolute_error: 0.2314\n",
      "\n",
      "Epoch 5/10\n",
      "195/195 [==============================]195/195 [==============================] - 179s 919ms/step - loss: 0.2660 - mean_squared_error: 0.2660 - mean_absolute_error: 0.2262\n",
      "\n",
      "Epoch 6/10\n",
      "195/195 [==============================]195/195 [==============================] - 172s 883ms/step - loss: 0.2596 - mean_squared_error: 0.2596 - mean_absolute_error: 0.2217\n",
      "\n",
      "Epoch 7/10\n",
      "195/195 [==============================]195/195 [==============================] - 172s 884ms/step - loss: 0.2484 - mean_squared_error: 0.2484 - mean_absolute_error: 0.2174\n",
      "\n",
      "Epoch 8/10\n",
      "195/195 [==============================]195/195 [==============================] - 172s 881ms/step - loss: 0.2376 - mean_squared_error: 0.2376 - mean_absolute_error: 0.2087\n",
      "\n",
      "Epoch 9/10\n",
      "195/195 [==============================]195/195 [==============================] - 175s 900ms/step - loss: 0.2370 - mean_squared_error: 0.2370 - mean_absolute_error: 0.2046\n",
      "\n",
      "Epoch 10/10\n",
      "195/195 [==============================]195/195 [==============================] - 179s 919ms/step - loss: 0.2430 - mean_squared_error: 0.2430 - mean_absolute_error: 0.2086\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x2a71ce77390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using all channels from HSV, Batch Size = 50, Conv # 1 = 5x5 with 3x3 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "195/195 [==============================]195/195 [==============================] - 178s 914ms/step - loss: 17.6068 - mean_squared_error: 17.6068 - mean_absolute_error: 0.9950\n",
      "\n",
      "Epoch 2/10\n",
      "195/195 [==============================]195/195 [==============================] - 184s 942ms/step - loss: 0.3048 - mean_squared_error: 0.3048 - mean_absolute_error: 0.2523\n",
      "\n",
      "Epoch 3/10\n",
      "195/195 [==============================]195/195 [==============================] - 177s 907ms/step - loss: 0.2909 - mean_squared_error: 0.2909 - mean_absolute_error: 0.2500\n",
      "\n",
      "Epoch 4/10\n",
      "195/195 [==============================]195/195 [==============================] - 179s 917ms/step - loss: 0.2779 - mean_squared_error: 0.2779 - mean_absolute_error: 0.2404\n",
      "\n",
      "Epoch 5/10\n",
      "195/195 [==============================]195/195 [==============================] - 182s 934ms/step - loss: 0.2717 - mean_squared_error: 0.2717 - mean_absolute_error: 0.2283\n",
      "\n",
      "Epoch 6/10\n",
      "195/195 [==============================]195/195 [==============================] - 177s 906ms/step - loss: 0.2558 - mean_squared_error: 0.2558 - mean_absolute_error: 0.2185\n",
      "\n",
      "Epoch 7/10\n",
      "195/195 [==============================]195/195 [==============================] - 176s 901ms/step - loss: 0.2451 - mean_squared_error: 0.2451 - mean_absolute_error: 0.2134\n",
      "\n",
      "Epoch 8/10\n",
      "195/195 [==============================]195/195 [==============================] - 190s 976ms/step - loss: 0.2487 - mean_squared_error: 0.2487 - mean_absolute_error: 0.2135\n",
      "\n",
      "Epoch 9/10\n",
      "195/195 [==============================]195/195 [==============================] - 193s 989ms/step - loss: 0.2298 - mean_squared_error: 0.2298 - mean_absolute_error: 0.2016\n",
      "\n",
      "Epoch 10/10\n",
      "195/195 [==============================]195/195 [==============================] - 189s 970ms/step - loss: 0.2245 - mean_squared_error: 0.2245 - mean_absolute_error: 0.1968\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x244879e7748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (5,5), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Conv # 1 and pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uing all channels from HSV, Batch Size = 30, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "325/325 [==============================]325/325 [==============================] - 416s 1s/step - loss: 3.5330 - mean_squared_error: 3.5330 - mean_absolute_error: 0.4980\n",
      "\n",
      "Epoch 2/10\n",
      "325/325 [==============================]325/325 [==============================] - 425s 1s/step - loss: 0.2964 - mean_squared_error: 0.2964 - mean_absolute_error: 0.2509\n",
      "\n",
      "Epoch 3/10\n",
      "325/325 [==============================]325/325 [==============================] - 398s 1s/step - loss: 0.2901 - mean_squared_error: 0.2901 - mean_absolute_error: 0.2491\n",
      "\n",
      "Epoch 4/10\n",
      "325/325 [==============================]325/325 [==============================] - 393s 1s/step - loss: 0.2842 - mean_squared_error: 0.2842 - mean_absolute_error: 0.2399\n",
      "\n",
      "Epoch 5/10\n",
      "325/325 [==============================]325/325 [==============================] - 410s 1s/step - loss: 0.2755 - mean_squared_error: 0.2755 - mean_absolute_error: 0.2345\n",
      "\n",
      "Epoch 6/10\n",
      "325/325 [==============================]325/325 [==============================] - 392s 1s/step - loss: 0.2606 - mean_squared_error: 0.2606 - mean_absolute_error: 0.2239\n",
      "\n",
      "Epoch 7/10\n",
      "325/325 [==============================]325/325 [==============================] - 401s 1s/step - loss: 0.2536 - mean_squared_error: 0.2536 - mean_absolute_error: 0.2167\n",
      "\n",
      "Epoch 8/10\n",
      "325/325 [==============================]325/325 [==============================] - 401s 1s/step - loss: 0.2421 - mean_squared_error: 0.2421 - mean_absolute_error: 0.2115\n",
      "\n",
      "Epoch 9/10\n",
      "325/325 [==============================]325/325 [==============================] - 390s 1s/step - loss: 0.2535 - mean_squared_error: 0.2535 - mean_absolute_error: 0.2124\n",
      "\n",
      "Epoch 10/10\n",
      "325/325 [==============================]325/325 [==============================] - 403s 1s/step - loss: 0.2379 - mean_squared_error: 0.2379 - mean_absolute_error: 0.2045\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x24488a30240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uing first channel from HSV, Batch Size = 30, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 5x5 with 3x3 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 322s 928ms/step - loss: 0.3013 - mean_squared_error: 0.3013 - mean_absolute_error: 0.2477 - val_loss: 0.2552 - val_mean_squared_error: 0.2552 - val_mean_absolute_error: 0.2248\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 330s 950ms/step - loss: 0.2839 - mean_squared_error: 0.2839 - mean_absolute_error: 0.2353 - val_loss: 0.2412 - val_mean_squared_error: 0.2412 - val_mean_absolute_error: 0.1863\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 333s 959ms/step - loss: 0.2662 - mean_squared_error: 0.2662 - mean_absolute_error: 0.2196 - val_loss: 0.2281 - val_mean_squared_error: 0.2281 - val_mean_absolute_error: 0.1767\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 324s 935ms/step - loss: 0.2544 - mean_squared_error: 0.2544 - mean_absolute_error: 0.2128 - val_loss: 0.2626 - val_mean_squared_error: 0.2626 - val_mean_absolute_error: 0.1863\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 385s 1s/step - loss: 0.2495 - mean_squared_error: 0.2495 - mean_absolute_error: 0.2096 - val_loss: 0.2528 - val_mean_squared_error: 0.2528 - val_mean_absolute_error: 0.2410\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 423s 1s/step - loss: 0.2355 - mean_squared_error: 0.2355 - mean_absolute_error: 0.1985 - val_loss: 0.2761 - val_mean_squared_error: 0.2761 - val_mean_absolute_error: 0.2573\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 430s 1s/step - loss: 0.2238 - mean_squared_error: 0.2238 - mean_absolute_error: 0.1961 - val_loss: 0.2320 - val_mean_squared_error: 0.2320 - val_mean_absolute_error: 0.2090\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 350s 1s/step - loss: 0.2304 - mean_squared_error: 0.2304 - mean_absolute_error: 0.1967 - val_loss: 0.2934 - val_mean_squared_error: 0.2934 - val_mean_absolute_error: 0.2750\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 325s 936ms/step - loss: 0.2094 - mean_squared_error: 0.2094 - mean_absolute_error: 0.1863 - val_loss: 0.2810 - val_mean_squared_error: 0.2810 - val_mean_absolute_error: 0.2842\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 325s 938ms/step - loss: 0.2007 - mean_squared_error: 0.2007 - mean_absolute_error: 0.1855 - val_loss: 0.2838 - val_mean_squared_error: 0.2838 - val_mean_absolute_error: 0.2373\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cbdc42f080>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uing first channel from HSV, Batch Size = 30, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 5x5 with 3x3 pooling, all conv layers with 64 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "347/347 [==============================]347/347 [==============================] - 771s 2s/step - loss: 0.3043 - mean_squared_error: 0.3043 - mean_absolute_error: 0.2497 - val_loss: 0.2614 - val_mean_squared_error: 0.2614 - val_mean_absolute_error: 0.2308\n",
      "\n",
      "Epoch 2/10\n",
      "347/347 [==============================]347/347 [==============================] - 761s 2s/step - loss: 0.2866 - mean_squared_error: 0.2866 - mean_absolute_error: 0.2411 - val_loss: 0.2598 - val_mean_squared_error: 0.2598 - val_mean_absolute_error: 0.2330\n",
      "\n",
      "Epoch 3/10\n",
      "347/347 [==============================]347/347 [==============================] - 761s 2s/step - loss: 0.2773 - mean_squared_error: 0.2773 - mean_absolute_error: 0.2302 - val_loss: 0.2559 - val_mean_squared_error: 0.2559 - val_mean_absolute_error: 0.2468\n",
      "\n",
      "Epoch 4/10\n",
      "347/347 [==============================]347/347 [==============================] - 769s 2s/step - loss: 0.2725 - mean_squared_error: 0.2725 - mean_absolute_error: 0.2322 - val_loss: 0.2542 - val_mean_squared_error: 0.2542 - val_mean_absolute_error: 0.2597\n",
      "\n",
      "Epoch 5/10\n",
      "347/347 [==============================]347/347 [==============================] - 763s 2s/step - loss: 0.2653 - mean_squared_error: 0.2653 - mean_absolute_error: 0.2281 - val_loss: 0.2374 - val_mean_squared_error: 0.2374 - val_mean_absolute_error: 0.2393\n",
      "\n",
      "Epoch 6/10\n",
      "347/347 [==============================]347/347 [==============================] - 767s 2s/step - loss: 0.2759 - mean_squared_error: 0.2759 - mean_absolute_error: 0.2315 - val_loss: 0.2347 - val_mean_squared_error: 0.2347 - val_mean_absolute_error: 0.2463\n",
      "\n",
      "Epoch 7/10\n",
      "347/347 [==============================]347/347 [==============================] - 773s 2s/step - loss: 0.2697 - mean_squared_error: 0.2697 - mean_absolute_error: 0.2289 - val_loss: 0.2337 - val_mean_squared_error: 0.2337 - val_mean_absolute_error: 0.2288\n",
      "\n",
      "Epoch 8/10\n",
      "347/347 [==============================]347/347 [==============================] - 794s 2s/step - loss: 0.2588 - mean_squared_error: 0.2588 - mean_absolute_error: 0.2245 - val_loss: 0.2312 - val_mean_squared_error: 0.2312 - val_mean_absolute_error: 0.2393\n",
      "\n",
      "Epoch 9/10\n",
      "347/347 [==============================]347/347 [==============================] - 796s 2s/step - loss: 0.2454 - mean_squared_error: 0.2454 - mean_absolute_error: 0.2119 - val_loss: 0.2209 - val_mean_squared_error: 0.2209 - val_mean_absolute_error: 0.2363\n",
      "\n",
      "Epoch 10/10\n",
      "347/347 [==============================]347/347 [==============================] - 784s 2s/step - loss: 0.2459 - mean_squared_error: 0.2459 - mean_absolute_error: 0.2127 - val_loss: 0.2433 - val_mean_squared_error: 0.2433 - val_mean_absolute_error: 0.2696\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x1cc0a774f98>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.BatchNormalization(input_shape=(108,192,1)),\n",
    "  tf.keras.layers.Conv2D(64, (10,10), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(64, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10, \n",
    "                    validation_data=gather_images(test_X, test_Y, batch_size=batch_size), \n",
    "                    validation_steps = len(test_X)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing Conv 1 and 2 and pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses all channels from HSV, Batch Size = 30, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 10x10 with 5x5 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "325/325 [==============================]325/325 [==============================] - 412s 1s/step - loss: 1.7279 - mean_squared_error: 1.7279 - mean_absolute_error: 0.3966\n",
      "\n",
      "Epoch 2/10\n",
      "325/325 [==============================]325/325 [==============================] - 406s 1s/step - loss: 0.2950 - mean_squared_error: 0.2950 - mean_absolute_error: 0.2484\n",
      "\n",
      "Epoch 3/10\n",
      "325/325 [==============================]325/325 [==============================] - 416s 1s/step - loss: 0.2912 - mean_squared_error: 0.2912 - mean_absolute_error: 0.2432\n",
      "\n",
      "Epoch 4/10\n",
      "325/325 [==============================]325/325 [==============================] - 427s 1s/step - loss: 0.2857 - mean_squared_error: 0.2857 - mean_absolute_error: 0.2410\n",
      "\n",
      "Epoch 5/10\n",
      "325/325 [==============================]325/325 [==============================] - 423s 1s/step - loss: 0.2765 - mean_squared_error: 0.2765 - mean_absolute_error: 0.2344\n",
      "\n",
      "Epoch 6/10\n",
      "325/325 [==============================]325/325 [==============================] - 415s 1s/step - loss: 0.2930 - mean_squared_error: 0.2930 - mean_absolute_error: 0.2386\n",
      "\n",
      "Epoch 7/10\n",
      "325/325 [==============================]325/325 [==============================] - 406s 1s/step - loss: 0.2849 - mean_squared_error: 0.2849 - mean_absolute_error: 0.2377\n",
      "\n",
      "Epoch 8/10\n",
      "325/325 [==============================]325/325 [==============================] - 407s 1s/step - loss: 0.2771 - mean_squared_error: 0.2771 - mean_absolute_error: 0.2336\n",
      "\n",
      "Epoch 9/10\n",
      "325/325 [==============================]325/325 [==============================] - 409s 1s/step - loss: 0.2717 - mean_squared_error: 0.2717 - mean_absolute_error: 0.2342\n",
      "\n",
      "Epoch 10/10\n",
      "325/325 [==============================]325/325 [==============================] - 410s 1s/step - loss: 0.2614 - mean_squared_error: 0.2614 - mean_absolute_error: 0.2321\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x24487b2e358>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (10,10), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses all channels from HSV, Batch Size = 50, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 10x10 with 5x5 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "195/195 [==============================]195/195 [==============================] - 405s 2s/step - loss: 2.9963 - mean_squared_error: 2.9963 - mean_absolute_error: 0.4759\n",
      "\n",
      "Epoch 2/10\n",
      "195/195 [==============================]195/195 [==============================] - 400s 2s/step - loss: 0.2990 - mean_squared_error: 0.2990 - mean_absolute_error: 0.2558\n",
      "\n",
      "Epoch 3/10\n",
      "195/195 [==============================]195/195 [==============================] - 405s 2s/step - loss: 0.2997 - mean_squared_error: 0.2997 - mean_absolute_error: 0.2579\n",
      "\n",
      "Epoch 4/10\n",
      "195/195 [==============================]195/195 [==============================] - 434s 2s/step - loss: 0.2995 - mean_squared_error: 0.2995 - mean_absolute_error: 0.2580\n",
      "\n",
      "Epoch 5/10\n",
      "195/195 [==============================]195/195 [==============================] - 433s 2s/step - loss: 0.2990 - mean_squared_error: 0.2990 - mean_absolute_error: 0.2580\n",
      "\n",
      "Epoch 6/10\n",
      "195/195 [==============================]195/195 [==============================] - 415s 2s/step - loss: 0.2990 - mean_squared_error: 0.2990 - mean_absolute_error: 0.2575\n",
      "\n",
      "Epoch 7/10\n",
      "195/195 [==============================]195/195 [==============================] - 416s 2s/step - loss: 0.3000 - mean_squared_error: 0.3000 - mean_absolute_error: 0.2580\n",
      "\n",
      "Epoch 8/10\n",
      "195/195 [==============================]195/195 [==============================] - 406s 2s/step - loss: 0.2989 - mean_squared_error: 0.2989 - mean_absolute_error: 0.2574\n",
      "\n",
      "Epoch 9/10\n",
      "195/195 [==============================]195/195 [==============================] - 406s 2s/step - loss: 0.2985 - mean_squared_error: 0.2985 - mean_absolute_error: 0.2585\n",
      "\n",
      "Epoch 10/10\n",
      "195/195 [==============================]195/195 [==============================] - 425s 2s/step - loss: 0.2986 - mean_squared_error: 0.2986 - mean_absolute_error: 0.2573\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x24487b92908>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (10,10), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses all channels from HSV, Batch Size = 30, Conv # 1 = 10x10 with 5x5 pooling, Conv # 2 = 10x10 with 5x5 pooling, Conv # 3 = 5x5 with 2x2 pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "325/325 [==============================]325/325 [==============================] - 474s 1s/step - loss: 0.5884 - mean_squared_error: 0.5884 - mean_absolute_error: 0.2957\n",
      "\n",
      "Epoch 2/10\n",
      "325/325 [==============================]325/325 [==============================] - 426s 1s/step - loss: 0.2975 - mean_squared_error: 0.2975 - mean_absolute_error: 0.2553\n",
      "\n",
      "Epoch 3/10\n",
      "325/325 [==============================]325/325 [==============================] - 422s 1s/step - loss: 0.2962 - mean_squared_error: 0.2962 - mean_absolute_error: 0.2539\n",
      "\n",
      "Epoch 4/10\n",
      "325/325 [==============================]325/325 [==============================] - 421s 1s/step - loss: 0.2976 - mean_squared_error: 0.2976 - mean_absolute_error: 0.2548\n",
      "\n",
      "Epoch 5/10\n",
      "325/325 [==============================]325/325 [==============================] - 438s 1s/step - loss: 0.2969 - mean_squared_error: 0.2969 - mean_absolute_error: 0.2577\n",
      "\n",
      "Epoch 6/10\n",
      "325/325 [==============================]325/325 [==============================] - 426s 1s/step - loss: 0.2969 - mean_squared_error: 0.2969 - mean_absolute_error: 0.2567\n",
      "\n",
      "Epoch 7/10\n",
      "325/325 [==============================]325/325 [==============================] - 430s 1s/step - loss: 0.2965 - mean_squared_error: 0.2965 - mean_absolute_error: 0.2559\n",
      "\n",
      "Epoch 8/10\n",
      "325/325 [==============================]325/325 [==============================] - 430s 1s/step - loss: 0.2971 - mean_squared_error: 0.2971 - mean_absolute_error: 0.2571\n",
      "\n",
      "Epoch 9/10\n",
      "325/325 [==============================]325/325 [==============================] - 443s 1s/step - loss: 0.2968 - mean_squared_error: 0.2968 - mean_absolute_error: 0.2564\n",
      "\n",
      "Epoch 10/10\n",
      "325/325 [==============================]325/325 [==============================] - 441s 1s/step - loss: 0.2974 - mean_squared_error: 0.2974 - mean_absolute_error: 0.2574\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x24487c28908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "\n",
    "# Set up Convolutional Network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(8, (10,10), padding=\"same\",input_shape=(108, 192, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(16, (10,10), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(5,5)),\n",
    "  tf.keras.layers.Conv2D(32, (5,5), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',\n",
    "              metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "\n",
    "model.fit_generator(gather_images(train_X, train_Y, batch_size=batch_size),\n",
    "                    steps_per_epoch = len(train_X)//batch_size, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_notebook",
   "language": "python",
   "name": "tensor_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
